{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task\n",
        "\n",
        "Машинный перевод на базе декодера-энкодера RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programming\\projects\\airfiols\\airfoil\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MIEGXF8oM9tt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['translator.ipynb', 'Лекция 9.ipynb', 'data', 'venv']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['eng-fra.txt', 'names']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/eng-fra.txt', 'rt', encoding='utf-8') as file:\n",
        "    text = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Five tremors in excess of magnitude 5.0 on the Richter scale have shaken Japan just this week, but scientists are warning that the largest expected aftershock has yet to hit.\\tCinq secousses dépassant la magnitude cinq sur l'échelle de Richter ont secoué le Japon précisément cette semaine, mais les scientifiques avertissent que la plus grande réplique est encore à venir.\\n\",\n",
              " \"No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\\tPeu importe le temps que tu passeras à essayer de convaincre les gens que le chocolat est de la vanille, ça restera toujours du chocolat, même si tu réussis à convaincre toi et quelques autres que c'est de la vanille.\\n\",\n",
              " \"A child who is a native speaker usually knows many things about his or her language that a non-native speaker who has been studying for years still does not know and perhaps will never know.\\tUn enfant qui est un locuteur natif connaît habituellement de nombreuses choses sur son langage qu'un locuteur non-natif qui a étudié pendant des années ignore encore et peut-être ne saura jamais.\\n\",\n",
              " \"There are four main causes of alcohol-related death. Injury from car accidents or violence is one. Diseases like cirrhosis of the liver, cancer, heart and blood system diseases are the others.\\tIl y a quatre causes principales de décès liés à l'alcool. Les blessures dans les accidents automobiles ou la violence en est une. Les maladies comme la cirrhose, le cancer, les maladies cardio-vasculaires en sont les autres.\\n\",\n",
              " '\"Top-down economics never works,\" said Obama. \"The country does not succeed when just those at the very top are doing well. We succeed when the middle class gets bigger, when it feels greater security.\"\\t« L\\'économie en partant du haut vers le bas, ça ne marche jamais, » a dit Obama. « Le pays ne réussit pas lorsque seulement ceux qui sont au sommet s\\'en sortent bien. Nous réussissons lorsque la classe moyenne s\\'élargit, lorsqu\\'elle se sent davantage en sécurité. »\\n',\n",
              " \"A carbon footprint is the amount of carbon dioxide pollution that we produce as a result of our activities. Some people try to reduce their carbon footprint because they are concerned about climate change.\\tUne empreinte carbone est la somme de pollution au dioxyde de carbone que nous produisons par nos activités. Certaines personnes essaient de réduire leur empreinte carbone parce qu'elles sont inquiètes du changement climatique.\\n\",\n",
              " \"Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\\tLa mort est une chose qu'on nous décourage souvent de discuter ou même de penser mais j'ai pris conscience que se préparer à la mort est l'une des choses que nous puissions faire qui nous investit le plus de responsabilité. Réfléchir à la mort clarifie notre vie.\\n\",\n",
              " \"Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\\tPuisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arrière lorsque j'atterris sur n'importe quelle page qui contient des publicités surgissantes. Je me rends juste sur la prochaine page proposée par Google et espère tomber sur quelque chose de moins irritant.\\n\",\n",
              " \"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\\tSi quelqu'un qui ne connaît pas vos antécédents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarqué quelque chose à propos de votre élocution qui l'a fait prendre conscience que vous n'êtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\\n\",\n",
              " \"It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\\tIl est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\\n\"]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kyNnJyruM9t1"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name # language\n",
        "        self.word2index = {} # индекс слова\n",
        "        self.word2count = {} # счетчик слова\n",
        "        # токен слова\n",
        "        self.index2word = {0: \"SOS\", # starts of sentence\n",
        "                           1: \"EOS\"} # end of sentence\n",
        "        self.n_words = 2  # Count SOS and EOS количество слов\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        \"\"\"Добавление предложения\"\"\"\n",
        "        for word in sentence.split(' '): # разбивка через пробел\n",
        "            self.addWord(word) # добавление слова\n",
        "\n",
        "    def addWord(self, word):\n",
        "        \"\"\"Добавление слова\"\"\"\n",
        "        if word not in self.word2index: # при новом слове\n",
        "            self.word2index[word] = self.n_words # индекс слова\n",
        "            self.word2count[word] = 1  # количетво слов \n",
        "            self.index2word[self.n_words] = word # слово по индексу\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1  # величение счетчика данного слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FXKs8j4bM9t6"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalizeString(s):\n",
        "    \"\"\"нормализация текста путем перевода в нижний регистр и обрезки краевых пробелов\"\"\"\n",
        "    s = unicodeToAscii(s.lower().strip()) \n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # между данными знаками пунктуации вставляем пробел\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) # \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D8T4VxZeM9t-"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    with open(f'data/{lang1}-{lang2}.txt', encoding='utf-8') as file:\n",
        "        lines = file.read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "сокращаем использование памяти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eBOwgEBdM9uB"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "# сокращения\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p) -> bool:\n",
        "    return all((len(p[0].split(' ')) < MAX_LENGTH, len(p[1].split(' ')) < MAX_LENGTH, p[1].startswith(eng_prefixes)))\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    \"\"\"Фильтрация пар\"\"\"\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "colab_type": "code",
        "id": "6dZOGjd5M9uE",
        "outputId": "0cdd3a7f-2ac8-4872-8a81-6101d0bdd0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 135842 sentence pairs\n",
            "Trimmed to 10853 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4489\n",
            "eng 2925\n",
            "['j apprends vite .', 'i m a fast learner .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vgtWqznCM9uH"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "m9vm9QBWM9uI"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size) # слой эмбедингов\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size) # ячейка gru\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # получаем эмбединг и приводим к нужному виду\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden) # проброс в gru\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device) # заполнение нулями"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwLTlgSyM9uK"
      },
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PFbuUL1LM9uL"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size) # скрытое представление в \n",
        "        self.softmax = nn.LogSoftmax(dim=1) # нормирование чтобы суммирование в единицу\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1) # получаем эмбединг и правим его размерность\n",
        "        output = F.relu(output) # ф-я активации\n",
        "        output, hidden = self.gru(output, hidden) # gru\n",
        "        output = self.softmax(self.out(output[0])) # поносвязный слой\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        \"\"\"Функция инициализации скрытого состояния\"\"\"\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z6gGPtXFM9uQ"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    \"\"\"Функция перевода предложения в последовательность индексов\"\"\"\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    \"\"\"Функция преобразование предлоения в тензор\"\"\"\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token) # конец предложения\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    \"\"\"Функция перевода в тензор пары предложения\"\"\"\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8Fn8VDv8M9uS"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5 # в половине случаев будет случаться в половине -нет\n",
        "\n",
        "\n",
        "def train(input_tensor: torch.tensor, target_tensor: torch.tensor, \n",
        "          encoder, decoder, \n",
        "          encoder_optimizer, decoder_optimizer, \n",
        "          criterion, #loss function\n",
        "          max_length=MAX_LENGTH, # ограничение генерации\n",
        "          ):\n",
        "    \"\"\"Получение на 1 предложении\"\"\"\n",
        "    encoder_hidden = encoder.initHidden() # первое скрытое представление\n",
        "\n",
        "    # сброс градиентов\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # тензор из индексов входного и выходного предложения\n",
        "    input_length, target_length = input_tensor.size(0), target_tensor.size(0) \n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) # хранилище\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length): # пропускание по слову через encoder\n",
        "        # обновленный контекс и представление очередного слова\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], # индекс токена\n",
        "                                                 encoder_hidden) # текущее представление\n",
        "        encoder_outputs[ei] = encoder_output[0, 0] # пусть лежит тут\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device) # стартовый токен\n",
        "\n",
        "    decoder_hidden = encoder_hidden # передача скрытого рпедставления из encoder в decoder\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False # применение teacher_forcing_ratio или нет\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length): # бежим по целевому тензору\n",
        "            # распределение вероятности следующего слова и скрытый слой после генерации для последующих слов\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1) # топ 1 лучшего токена слова\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token: break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EKsdwPmSM9uU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = s // 60\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C_z_k5IiM9uX"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, \n",
        "               n_iters, # эпохи\n",
        "               print_every=1_000, plot_every=100, learning_rate=0.01):\n",
        "    \"\"\"Тренировка на парах\"\"\"\n",
        "    start = time.time()\n",
        "    plot_losses = [] # ошибки для прорисовки\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) # оптимизаторы\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)] # тензоры пар\n",
        "    criterion = nn.NLLLoss() # негативный логарифм правдоподобия\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1] # выбор пары\n",
        "        input_tensor = training_pair[0] # выходной тензор\n",
        "        target_tensor = training_pair[1] # выходной тензор\n",
        " \n",
        "        loss = train(input_tensor, target_tensor, \n",
        "                     encoder, decoder, \n",
        "                     encoder_optimizer, decoder_optimizer, \n",
        "                     criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3Bxf45h6M9ud"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    \"\"\"Применение модели для перевода\"\"\"\n",
        "    with torch.no_grad(): # отключение градиента\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence) # перевод в тензор\n",
        "        input_length = input_tensor.size()[0] \n",
        "        encoder_hidden = encoder.initHidden() # инициализоруем скрытое представление\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) # инициализация срытого представления\n",
        "\n",
        "        for ei in range(input_length): # прогоняем данные через encoder\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # создаем стартовый токен SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden # передача скрытого состояния из encoder в decoder\n",
        "\n",
        "        decoded_words = [] # пустой список сгенерированных слов\n",
        "\n",
        "        for di in range(max_length): # генерация до максимально предела по количеству\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1) # топ 1 по вероятности индекс слова\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()]) # получение слова по индексу\n",
        "\n",
        "            decoder_input = topi.squeeze().detach() # отброс единичной размерности и отбрасываем градиенты\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1qUmQIGwM9uf"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    \"\"\"Демнтсрация\"\"\"\n",
        "    for _ in range(n):\n",
        "        pair = random.choice(pairs) # берет случайную пару \n",
        "        print('>', pair[0]) # входное предложение\n",
        "        print('=', pair[1]) # выходное предложение\n",
        "        output_words = evaluate(encoder, decoder, pair[0]) # перевод предложения\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence) # печать сгенерированного рпедложения\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "colab_type": "code",
        "id": "s_56t10oM9uh",
        "outputId": "f456b0b8-fc35-4199-fb19-b45c2330bf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 56s (- 27m 16s) (5000 6%) 2.9716\n",
            "3m 40s (- 23m 52s) (10000 13%) 2.3972\n",
            "5m 26s (- 21m 44s) (15000 20%) 2.0621\n",
            "7m 17s (- 20m 4s) (20000 26%) 1.8393\n",
            "8m 59s (- 17m 58s) (25000 33%) 1.5858\n",
            "10m 47s (- 16m 11s) (30000 40%) 1.4390\n",
            "12m 35s (- 14m 23s) (35000 46%) 1.2647\n",
            "14m 28s (- 12m 39s) (40000 53%) 1.1325\n",
            "16m 16s (- 10m 50s) (45000 60%) 1.0142\n",
            "18m 10s (- 9m 5s) (50000 66%) 0.9339\n",
            "20m 6s (- 7m 18s) (55000 73%) 0.8228\n",
            "21m 59s (- 5m 29s) (60000 80%) 0.7615\n",
            "23m 51s (- 3m 40s) (65000 86%) 0.6448\n",
            "25m 45s (- 1m 50s) (70000 93%) 0.5902\n",
            "27m 37s (- 0m 0s) (75000 100%) 0.5340\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 75_000, print_every=5_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xEoEylSyM9uj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> quel idiot vous etes !\n",
            "= you re such an idiot !\n",
            "< you re such an idiot ! <EOS>\n",
            "\n",
            "> nous en avons fini ici .\n",
            "= we re finished here .\n",
            "< we re finished here . <EOS>\n",
            "\n",
            "> ils preparent un mauvais tour .\n",
            "= they re up to no good .\n",
            "< they re up a good . <EOS>\n",
            "\n",
            "> vous etes grognon .\n",
            "= you re grumpy .\n",
            "< you re grumpy . <EOS>\n",
            "\n",
            "> nous n en sommes pas fiers .\n",
            "= we re not proud of it .\n",
            "< we re not proud of it . <EOS>\n",
            "\n",
            "> il se plaint tout le temps .\n",
            "= he is always complaining .\n",
            "< he is always complaining . <EOS>\n",
            "\n",
            "> elle est fiere de sa fille .\n",
            "= she is proud of her daughter .\n",
            "< she is proud of her daughter . <EOS>\n",
            "\n",
            "> tu es tres talentueux .\n",
            "= you re very talented .\n",
            "< you re very talented . <EOS>\n",
            "\n",
            "> il est tres deprime .\n",
            "= he is very depressed .\n",
            "< he is very depressed . <EOS>\n",
            "\n",
            "> je plaisante .\n",
            "= i am joking .\n",
            "< i m joking . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mechanism attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length) # Механизм Внимания\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) # слой агрегации информации\n",
        "        self.dropout = nn.Dropout(self.dropout_p) # предотвращение переобучения\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, # входное слово\n",
        "                hidden, # скрытое представление с контекстом\n",
        "                encoder_outputs): # хранилище информации механизма внимания\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # получение ембединга и образмеривание\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # конкатенация и прогоняем через слой внимания и нормирование softmax\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) \n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0) # агрегация информации весов внимания с исходным эмбедингом\n",
        "\n",
        "        output = F.relu(output) # ф-я активации\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1) # рспределение вероятности\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        \"\"\"Функция инициализации скрытого состояния\"\"\"\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1_000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length) # складывание векторов внимания после слова\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data # кладем вектор внимание в хранилище\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()]) # добавление слова из словаря\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2m 21s (- 32m 56s) (5000 6%) 2.9029\n",
            "4m 34s (- 29m 47s) (10000 13%) 2.3525\n",
            "6m 51s (- 27m 24s) (15000 20%) 2.0323\n",
            "9m 13s (- 25m 22s) (20000 26%) 1.7840\n",
            "11m 33s (- 23m 6s) (25000 33%) 1.6505\n",
            "14m 5s (- 21m 8s) (30000 40%) 1.4350\n",
            "16m 25s (- 18m 46s) (35000 46%) 1.2829\n",
            "18m 46s (- 16m 25s) (40000 53%) 1.1732\n",
            "21m 12s (- 14m 8s) (45000 60%) 1.0390\n",
            "23m 38s (- 11m 49s) (50000 66%) 0.9578\n",
            "26m 0s (- 9m 27s) (55000 73%) 0.8740\n",
            "28m 25s (- 7m 6s) (60000 80%) 0.8004\n",
            "30m 58s (- 4m 45s) (65000 86%) 0.7378\n",
            "33m 22s (- 2m 23s) (70000 93%) 0.6563\n",
            "36m 50s (- 0m 0s) (75000 100%) 0.6050\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75_000, print_every=5_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> je mange un sandwich .\n",
            "= i m eating a sandwich .\n",
            "< i m eating a sandwich . <EOS>\n",
            "\n",
            "> tu es incroyable .\n",
            "= you re amazing .\n",
            "< you re amazing . <EOS>\n",
            "\n",
            "> je suis sournois .\n",
            "= i m sneaky .\n",
            "< i m sneaky . <EOS>\n",
            "\n",
            "> vous n etes pas tres amusants .\n",
            "= you re not very funny .\n",
            "< you re not very funny . <EOS>\n",
            "\n",
            "> je suis americain .\n",
            "= i am american .\n",
            "< i am american . <EOS>\n",
            "\n",
            "> il va me l expliquer .\n",
            "= he is going to explain it to me .\n",
            "< he is going to me it . <EOS>\n",
            "\n",
            "> je suis a la prison .\n",
            "= i m at the prison .\n",
            "< i m in the prison . <EOS>\n",
            "\n",
            "> il se trouve en prison .\n",
            "= he s in prison .\n",
            "< he s in prison . <EOS>\n",
            "\n",
            "> je ne crains personne .\n",
            "= i m not afraid of anyone .\n",
            "< i m not afraid of you . <EOS>\n",
            "\n",
            "> elle lui a fait un proces .\n",
            "= she sued him .\n",
            "< she sued him . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\832941923.py:4: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "output_words, attentions = evaluate(encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input = elle a cinq ans de moins que moi .\n",
            "output = she is five years younger than me . <EOS>\n",
            "input = elle est trop petit .\n",
            "output = she is too short . <EOS>\n",
            "input = je ne crains pas de mourir .\n",
            "output = i m not scared to die . <EOS>\n",
            "input = c est un jeune directeur plein de talent .\n",
            "output = he s a talented young young . <EOS>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:11: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_yticklabels([''] + output_words)\n",
            "C:\\Users\\parkh\\AppData\\Local\\Temp\\ipykernel_23956\\744067545.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    \"\"\"Отрисовка внимания\"\"\"\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Лекция 8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
